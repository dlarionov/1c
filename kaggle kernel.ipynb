{
  "cells": [
    {
      "metadata": {
        "_uuid": "692a55996b5351079dcd065431be3a71ea763e6d"
      },
      "cell_type": "markdown",
      "source": "This notebook is simpified version of the final project in the [How to Win a Data Science Competition: Learn from Top Kagglers](https://www.coursera.org/learn/competitive-data-science) course, which I highly recommend.\n\n#### My pipline\n* load data\n* heal data and remove outliers\n* work with shops/items/cats objects and features\n* create tfidf features from shop_name\n* add revenue to the train set\n* create matrix as product of item/shop pairs within each month in the train set\n* get monthly sales/orders for each item/shop pair in the train set and merge it to the matrix\n* clip item_cnt_month by (0,20)\n* append test to the matrix, fill 34 month nans with zeros\n* merge shops/items/cats to the matrix\n* add target lag features by item_cnt_month, orders, revenue\n* add mean encoded features with lags by item_id, shop_id, category_id, city_id.\n* add price trend feature\n* add month\n* add days in month\n* add months since last sale, months since first sale features\n* cut first year and drop columns which can not be calculated for the test set\n* select best features\n* set validation strategy (34 test, 33 validation, less than 33 train)\n* fit the model, predict and clip targets for the test set\n\n#### TODO list\n* use TF-IDF features\n* create sales and returns columns based on item_cnt_day. sales for > 0 and returns for < 0. then aggregate both and use them as target values (item_sales_cnt_month and item_returns_cnt_month). build two models one for sales and another for returns. merge their predictions as a sum. I supose that predicting returns is easier and another model without returns probably will behave like without noise. For example, if no items were sold in the past means no items will be returned in the future.\n* create separate model for the shop_id=12 which is online shop and behave differ\n* classify items by is_digital property somehow. digital items should have zero sales on the phisical shops.\n* classify items by is_service property somehow. service is for example delivery or repairment, but not sale.\n* exclude items from test which are not present in the full train and predict zeros for them manualy\n* useregularization with mean encoding (reference to 3th week content)"
    },
    {
      "metadata": {
        "_uuid": "f03379ee467570732ebb2b3d20062fea0584d57d"
      },
      "cell_type": "markdown",
      "source": "# Part 1, Features"
    },
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport pickle\nimport time\nimport gc\nimport sys\nsys.version_info",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "items = pd.read_csv('../input/items.csv')\nshops = pd.read_csv('../input/shops.csv')\ncats = pd.read_csv('../input/item_categories.csv')\ntrain = pd.read_csv('../input/sales_train.csv')\ntest  = pd.read_csv('../input/test.csv').set_index('ID')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ed7a190645750a818e29a6291ba2553a91764c7c"
      },
      "cell_type": "markdown",
      "source": "## Outliers"
    },
    {
      "metadata": {
        "_uuid": "425d8f2dc08378977b393bf80c5fdcf0fba2c992"
      },
      "cell_type": "markdown",
      "source": "There are items with strange prices and sales. After detailed exploration I decided to remove items with price > 100000 and sales > 1001 (1000 is ok)."
    },
    {
      "metadata": {
        "_uuid": "5a864412fafc3129a3e9bd5bb1f18a7cf0c62935",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7e621535d112603c60aeb2c2f83dbbf96d36b732",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train = train[train.item_price<100000]\ntrain = train[train.item_cnt_day<1001]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d2f99368478e3063b1c379537944e954d7186928"
      },
      "cell_type": "markdown",
      "source": "There is one item with price below zero. Fill it with median."
    },
    {
      "metadata": {
        "_uuid": "0fc6b90b22fe232f4240ac8f965cc52b3db5526a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "median = train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)].item_price.median()\ntrain.loc[train.item_price<0, 'item_price'] = median",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7da194c285d696b5c6978148bf0143b9b2a7b0c5"
      },
      "cell_type": "markdown",
      "source": "Several shops are duplicates of each other (according to its name). Fix train and test set."
    },
    {
      "metadata": {
        "_uuid": "00fe91e9c482ea413abd774ff903fe3d152785dd",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Якутск Орджоникидзе, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# Якутск ТЦ \"Центральный\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# Жуковский ул. Чкалова 39м²\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a30f0521464e1fa20444e66d24bbdcb76b93f6de"
      },
      "cell_type": "markdown",
      "source": "## Shops/Cats/Items preprocessing\nObservations:\n* Each shop_name starts with the city name.\n* Each category contains type and subtype in its name."
    },
    {
      "metadata": {
        "_uuid": "12fae4c8d0c8f3e817307d1e0ffc6831e9a8d696",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "shops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]\n\ncats['split'] = cats['item_category_name'].str.split('-')\ncats['type'] = cats['split'].map(lambda x: x[0].strip())\ncats['type_code'] = LabelEncoder().fit_transform(cats['type'])\n# if subtype is nan then type\ncats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\ncats = cats[['item_category_id','type_code', 'subtype_code']]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2e0da873166e895c2c981a9f38f2647cd3327784"
      },
      "cell_type": "markdown",
      "source": "TF-IDF features from item_name. "
    },
    {
      "metadata": {
        "_uuid": "7ec93dbe2c4126e6e8f129d4cd30cebb28ead3bf",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "tfidf = pd.DataFrame(TfidfVectorizer(max_features=12).fit_transform(items['item_name']).toarray(), dtype='float16')\ntfidf.columns = ['tfidf_'+str(id+1) for id in range(12)]\ntfidf.index.names = ['item_id']\ntfidf.reset_index(inplace=True)\n\nitems.drop(['item_name'], axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "62c5f83fa222595da99294f465ab28e80ce415e9"
      },
      "cell_type": "markdown",
      "source": "## Monthly sales\nTest set is a product of some shops and items within 34 month. There are 5100 items * 42 shops = 214200 pairs. 363 items are new compared to train. Hence, for the most of the items in the test set target value should be zero. \nIn the other hand train set contains only pairs which were sold or returned in the past. Tha main idea is to calculate monthly sales and <b>extend it with zero sales</b> for each unique pair within the month. This way train data will be similar to test data."
    },
    {
      "metadata": {
        "_uuid": "fb69350aef2c28cdb619e2532de1e24ab3c43899",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "len(list(set(test.item_id) - set(test.item_id).intersection(set(train.item_id)))), len(list(set(test.item_id))), len(test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7626c7455ea71b65894c6c866519df15080fa2ac",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "ts = time.time()\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)\ntime.time() - ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "867e91a7570dd78b4834f4f1a166e58f80b63f93"
      },
      "cell_type": "markdown",
      "source": "Aggregate train set by shop/item pairs to calculate target aggreagates. <b>Clip(0,20) target value.</b>\n\n<i>I use floats instead of ints for item_cnt_month and orders to avoid downcasting it after concatination with the test set later. If it would be int16, after concatination with NaN values it becomes int64, but foat16 becomes float16 even with NaNs.</i>"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "9fef5477060be7d2e6c85dcb79d8e18e6253f7dd"
      },
      "cell_type": "code",
      "source": "train['revenue'] = train['item_price'] *  train['item_cnt_day']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7dd27181918fc7df89676e24d72130d183929d2d",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "ts = time.time()\ntrain_agg = train.groupby(['date_block_num','shop_id','item_id']).agg({\n    'item_cnt_day': ['sum', 'count'],\n    'revenue': ['sum']\n})\ntrain_agg.columns = ['item_cnt_month', 'orders', 'revenue']\ntrain_agg.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, train_agg, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float16))\nmatrix['orders'] = matrix['orders'].fillna(0).astype(np.float16)\nmatrix['revenue'] = matrix['revenue'].fillna(0).astype(np.float32)\ntime.time() - ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "315bc6107a93f3926a64fd09ea9244e9281ee41f"
      },
      "cell_type": "markdown",
      "source": "## Test set\nTo use time tricks append test pairs to the matrix."
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "29d02bdb4fa768577607bf735b918ca81da85d41"
      },
      "cell_type": "code",
      "source": "test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "177fbbab94c8057d67d61357d29581248468a74d"
      },
      "cell_type": "code",
      "source": "ts = time.time()\nmatrix = pd.concat([matrix, test], ignore_index=True, keys=cols)\ntime.time() - ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "20e26b5f4a9097a151631c96747ee1b3da58d968"
      },
      "cell_type": "code",
      "source": "matrix.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "233e394a6cebf36ef002dc76fef8d430026a52b3"
      },
      "cell_type": "markdown",
      "source": "## Shops/Items/Cats features"
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "7dfd5df3e2bcaee4c312f3979736f52c40f2560f"
      },
      "cell_type": "code",
      "source": "ts = time.time()\nmatrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\nmatrix.fillna(0, inplace=True)\ntime.time() - ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8358b291fdc8e0e7d1b5700974803b3f104715f7"
      },
      "cell_type": "markdown",
      "source": "## Traget lags"
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "9cd7bcc7643ce4545475e8e6f80d09a979aac42d"
      },
      "cell_type": "code",
      "source": "def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "78bf7ece93ebc4629ad0e48cd6a9927788d8706d"
      },
      "cell_type": "code",
      "source": "ts = time.time()\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')\nmatrix = lag_feature(matrix, [1,2,3], 'orders')\nmatrix = lag_feature(matrix, [1,2,3], 'revenue')\ntime.time() - ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c67bf4dbcef884ffe9d19c65d37bc4de1f287ef6"
      },
      "cell_type": "markdown",
      "source": "## Mean encoded features"
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "763aca242154ea10fa0a62fffadb4ef90e9532d6"
      },
      "cell_type": "code",
      "source": "ts = time.time()\ngroup = matrix.groupby(['date_block_num']).agg({\n    'item_cnt_month': ['mean'],\n    'orders': ['mean'],\n    'revenue': ['mean'],\n})\ngroup.columns = [ 'date_avg_item_cnt', 'date_avg_orders', 'date_avg_revenue' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\nmatrix['date_avg_orders'] = matrix['date_avg_orders'].astype(np.float16)\nmatrix['date_avg_revenue'] = matrix['date_avg_revenue'].astype(np.float32)\n\nmatrix = lag_feature(matrix, [1], 'date_avg_item_cnt')\nmatrix = lag_feature(matrix, [1], 'date_avg_orders')\nmatrix = lag_feature(matrix, [1], 'date_avg_revenue')\n\nmatrix.drop(['date_avg_item_cnt','date_avg_orders','date_avg_revenue'], axis=1, inplace=True)\n\ntime.time() - ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fc9166c4e678ebb99d03566f1751b7d4b5c690d2",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id']).agg({\n    'item_cnt_month': ['mean' ],\n    'orders': ['sum']\n})\ngroup.columns = [ 'date_item_avg_item_cnt', 'date_item_sum_orders' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix['date_item_sum_orders'] = matrix['date_item_sum_orders'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_item_avg_item_cnt')\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_item_sum_orders')\n\nmatrix.drop(['date_item_avg_item_cnt','date_item_sum_orders'], axis=1, inplace=True)\n\ntime.time() - ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "73f2552c403c5f67bbf07f28d69efcc015d00f32"
      },
      "cell_type": "code",
      "source": "ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id']).agg({\n    'item_cnt_month': ['mean' ],\n    'orders': ['sum']\n})\ngroup.columns = [ 'date_shop_avg_item_cnt', 'date_shop_sum_orders' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\nmatrix['date_shop_sum_orders'] = matrix['date_shop_sum_orders'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_shop_avg_item_cnt')\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_shop_sum_orders')\n\nmatrix.drop(['date_shop_avg_item_cnt','date_shop_sum_orders'], axis=1, inplace=True)\n\ntime.time() - ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "c3948a9b206bc480b31385c29a713aa49747de19"
      },
      "cell_type": "code",
      "source": "ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_category_id']).agg({\n    'item_cnt_month': ['mean'],\n    'orders': ['sum']\n})\ngroup.columns = [ 'date_cat_avg_item_cnt', 'date_cat_sum_orders' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\nmatrix['date_cat_sum_orders'] = matrix['date_cat_sum_orders'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1,2,3], 'date_cat_avg_item_cnt')\nmatrix = lag_feature(matrix, [1,2,3], 'date_cat_sum_orders')\n\nmatrix.drop(['date_cat_avg_item_cnt','date_cat_sum_orders'], axis=1, inplace=True)\ntime.time() - ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "bf98335755692f0d7666eeac2db1961692f09a16"
      },
      "cell_type": "code",
      "source": "ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({\n    'item_cnt_month': ['mean'],\n    'orders': ['sum']\n})\ngroup.columns = [ 'date_shop_cat_avg_item_cnt', 'date_shop_cat_sum_orders' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nmatrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\nmatrix['date_shop_cat_sum_orders'] = matrix['date_shop_cat_sum_orders'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], 'date_shop_cat_avg_item_cnt')\nmatrix = lag_feature(matrix, [1], 'date_shop_cat_sum_orders')\n\nmatrix.drop(['date_shop_cat_avg_item_cnt','date_shop_cat_sum_orders'], axis=1, inplace=True)\ntime.time() - ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "87d57d01beb0830138dabae79b4022d4c6a9cc12"
      },
      "cell_type": "code",
      "source": "ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'city_code']).agg({\n    'item_cnt_month': ['mean'],\n    'orders': ['sum']\n})\ngroup.columns = [ 'date_city_avg_item_cnt', 'date_city_sum_orders' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'city_code'], how='left')\nmatrix['date_city_avg_item_cnt'] = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix['date_city_sum_orders'] = matrix['date_city_sum_orders'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], 'date_city_avg_item_cnt')\nmatrix = lag_feature(matrix, [1], 'date_city_sum_orders')\n\nmatrix.drop(['date_city_avg_item_cnt','date_city_sum_orders'], axis=1, inplace=True)\ntime.time() - ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "db1f0170ec4a6fd9894bc53b36f3166d4b26abcf"
      },
      "cell_type": "code",
      "source": "ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id', 'city_code']).agg({\n    'item_cnt_month': ['mean'],\n    'orders': ['sum']\n})\ngroup.columns = [ 'date_item_city_avg_item_cnt', 'date_item_city_sum_orders' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'city_code'], how='left')\nmatrix['date_item_city_avg_item_cnt'] = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\nmatrix['date_item_city_sum_orders'] = matrix['date_item_city_sum_orders'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], 'date_item_city_avg_item_cnt')\nmatrix = lag_feature(matrix, [1], 'date_item_city_sum_orders')\n\nmatrix.drop(['date_item_city_avg_item_cnt','date_item_city_sum_orders'], axis=1, inplace=True)\ntime.time() - ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "29d4ab4e1a9cb6f05f58d905b3cc7787d4eacc04"
      },
      "cell_type": "code",
      "source": "matrix.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6bcea31d93ab035ca3fa1ed7c0afddbf602c414a"
      },
      "cell_type": "markdown",
      "source": "## Price features"
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "5b6530d9f3e910db3e36988dd635507382206030"
      },
      "cell_type": "code",
      "source": "ts = time.time()\ngroup = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\ntime.time() - ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0504e9613087237c255914d9ebd165fac4e88cd0"
      },
      "cell_type": "markdown",
      "source": "Price trend for the last six months."
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "0da2ded8502e273137991fd2bebbadaf19c19622"
      },
      "cell_type": "code",
      "source": "ts = time.time()\ngroup = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n\nlags = [1,2,3,4,5,6]\nmatrix = lag_feature(matrix, [1,2,3,4,5,6], 'date_item_avg_item_price')\n\nfor i in lags:\n    matrix['delta_price_lag_'+str(i)] = \\\n        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) / matrix['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nmatrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\nmatrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\n\n# https://stackoverflow.com/questions/31828240/first-non-null-value-per-row-from-a-list-of-pandas-columns/31828559\n# matrix['price_trend'] = matrix[['delta_price_lag_1','delta_price_lag_2','delta_price_lag_3']].bfill(axis=1).iloc[:, 0]\n# Invalid dtype for backfill_2d [float16]\n\nfetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nmatrix.drop(fetures_to_drop, axis=1, inplace=True)\n\ntime.time() - ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "47e06af411b7d26cd93dad3d6735e48e5fbdee50"
      },
      "cell_type": "markdown",
      "source": "## Special features"
    },
    {
      "metadata": {
        "_uuid": "cc31371e98e4db446dddcd17e13c65d4b3596100"
      },
      "cell_type": "markdown",
      "source": "Month (0 - Jan, 11 - Dec)"
    },
    {
      "metadata": {
        "_uuid": "bb521e1f33d4124a3b90b47447bdb29150770b6e",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "matrix['month'] = matrix['date_block_num'] % 12",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b4dc4d2ff86483989c4b74fc02a0d01ca68a5c75"
      },
      "cell_type": "markdown",
      "source": "Number of days in a month. There are no leap years."
    },
    {
      "metadata": {
        "_uuid": "e23f0201056b73368e3b70d4c36c6bb9e4a55291",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmatrix['days'] = matrix['month'].map(days).astype(np.int8)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7c096e86eb0043c0f6eeb899de24e28ca4c4e044"
      },
      "cell_type": "markdown",
      "source": "Months since last sale for each shop/item pair and for item only. I use programing approach.\n\nCreate hash table with key equals to {shop_id,item_id} and value equals to date_block_num. Iterate data from the top. Foreach row if {row.shop_id,item_id} is not present in the table, then add to the table and set its value to row.date_blocl_num. if hash table contains key, then calculate the difference beteween cached value and row.date_block_num."
    },
    {
      "metadata": {
        "_uuid": "3458a7056c963167760921417d1f863f074f2b39",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "ts = time.time()\ncache = {}\nmatrix['item_shop_last_sale'] = -1\nmatrix['item_shop_last_sale'] = matrix['item_shop_last_sale'].astype(np.int8)\nfor idx, row in matrix.iterrows():    \n    key = str(row.item_id)+' '+str(row.shop_id)\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        matrix.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n        cache[key] = row.date_block_num         \ntime.time() - ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "28b29fae3906d870b4dc3064a7f359b6d3abf623",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "ts = time.time()\ncache = {}\nmatrix['item_last_sale'] = -1\nmatrix['item_last_sale'] = matrix['item_last_sale'].astype(np.int8)\nfor idx, row in matrix.iterrows():    \n    key = row.item_id\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        if row.date_block_num>last_date_block_num:\n            matrix.at[idx, 'item_last_sale'] = row.date_block_num - last_date_block_num\n            cache[key] = row.date_block_num         \ntime.time() - ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "61987e6adc1bec2ea897eec837c0253f7f73fdb5"
      },
      "cell_type": "markdown",
      "source": "Months since first sale for each shop/item pair and for item only."
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "ad0869709bbada35726d5ca41dd913d817249f8e"
      },
      "cell_type": "code",
      "source": "ts = time.time()\nmatrix['item_shop_first_sale'] = matrix['date_block_num'] \\\n    - matrix.groupby(['item_id', 'shop_id'])['date_block_num'].transform('min')\nmatrix['item_first_sale'] = matrix['date_block_num'] \\\n    - matrix.groupby('item_id')['date_block_num'].transform('min')\ntime.time() - ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c60aeb05f15e7d9cec69dcc6ccae8fc1f04b5242"
      },
      "cell_type": "markdown",
      "source": "## TF-IDF"
    },
    {
      "metadata": {
        "_uuid": "39820acdd37dc5bf04e7b65b17a1aea1542799e2",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# data = pd.merge(data, tfidf, on='item_id', how='left')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "966cb34ccfe849fbb3707d93270691cb8eef7a89"
      },
      "cell_type": "markdown",
      "source": "## Final preparations"
    },
    {
      "metadata": {
        "_uuid": "ee4629c6b8b800679952e8e27f180b3bc0d04899"
      },
      "cell_type": "markdown",
      "source": "Because of the using 12 as lag value drop first 12 months. Also drop all the columns with this month calculated values (other words which can not be calcucated for the test set)."
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "04df1bc4240f409a5d4521c6f70c2ced44f7c3d4"
      },
      "cell_type": "code",
      "source": "ts = time.time()\nmatrix = matrix[matrix.date_block_num > 11]\nmatrix.drop(['orders', 'revenue'], axis=1, inplace=True)\ntime.time() - ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "48a14784050901f878b40f093e4bc34e07ecce05"
      },
      "cell_type": "markdown",
      "source": "Producing lags brings a lot of nulls. But it doesnt matter."
    },
    {
      "metadata": {
        "_uuid": "8e5d8cb5cea9be28af4a0486cc1bf797e5b5c7ee",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "'''\nts = time.time()\ndef fill_na(df):\n    for col in df.columns:\n        if df[col].isnull().any():\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)\n            if ('orders' in col):\n                df[col].fillna(0, inplace=True)\n            if ('item_price' in col):\n                df[col].fillna(df[col].median(), inplace=True)\n            if ('revenue' in col):\n                df[col].fillna(df[col].median(), inplace=True)\n    return df\n\nmatrix = fill_na(matrix)\ntime.time() - ts\n'''",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07fb74b23020bf6bf4044db412f5bf69b2c2d02c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "matrix.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "71c40e5ba3710e43d6d60c3e82aaa326c44d4108"
      },
      "cell_type": "markdown",
      "source": "# Part 2, xgboost"
    },
    {
      "metadata": {
        "_uuid": "3260cdece059370ec94abef6defb911d5f374d08"
      },
      "cell_type": "markdown",
      "source": "Select best features and fit the model."
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "04468520745492d48b86f8f68f560ebb05b7818b"
      },
      "cell_type": "code",
      "source": "matrix = matrix[[\n    'date_block_num', \n    'item_id', \n    'shop_id',\n    'item_cnt_month',\n    'city_code',       \n    'item_category_id', \n    'type_code', \n    'subtype_code',\n    'item_cnt_month_lag_1',       \n    'item_cnt_month_lag_2', \n    'item_cnt_month_lag_3', \n    'item_cnt_month_lag_6',\n    'item_cnt_month_lag_12', \n    #'orders_lag_1', \n    #'orders_lag_2', \n    #'orders_lag_3',\n    #'revenue_lag_1', \n    #'revenue_lag_2', \n    #'revenue_lag_3',\n    'date_avg_item_cnt_lag_1', \n    #'date_avg_orders_lag_1',\n    #'date_avg_revenue_lag_1', \n    'date_item_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_2', \n    'date_item_avg_item_cnt_lag_3',\n    'date_item_avg_item_cnt_lag_6', \n    'date_item_avg_item_cnt_lag_12',\n    #'date_item_sum_orders_lag_1', \n    #'date_item_sum_orders_lag_2',\n    #'date_item_sum_orders_lag_3', \n    #'date_item_sum_orders_lag_6',\n    #'date_item_sum_orders_lag_12', \n    'date_shop_avg_item_cnt_lag_1',\n    'date_shop_avg_item_cnt_lag_2', \n    'date_shop_avg_item_cnt_lag_3',\n    'date_shop_avg_item_cnt_lag_6', \n    'date_shop_avg_item_cnt_lag_12',\n    #'date_shop_sum_orders_lag_1', \n    #'date_shop_sum_orders_lag_2',\n    #'date_shop_sum_orders_lag_3', \n    #'date_shop_sum_orders_lag_6',\n    #'date_shop_sum_orders_lag_12', \n    'date_cat_avg_item_cnt_lag_1',\n    'date_cat_avg_item_cnt_lag_2', \n    'date_cat_avg_item_cnt_lag_3',\n    #'date_cat_sum_orders_lag_1', \n    #'date_cat_sum_orders_lag_2',\n    #'date_cat_sum_orders_lag_3', \n    'date_shop_cat_avg_item_cnt_lag_1',\n    'date_shop_cat_sum_orders_lag_1', \n    'date_city_avg_item_cnt_lag_1',\n    'date_city_sum_orders_lag_1', \n    'date_item_city_avg_item_cnt_lag_1',\n    'date_item_city_sum_orders_lag_1',\n    'delta_price_lag', \n    'month',\n    'days', \n    'item_shop_last_sale',\n    'item_last_sale',\n    'item_shop_first_sale', \n    'item_first_sale'\n]]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "00bf3fffc1b143d0555d03b9d79b5fd00d9d0dc9"
      },
      "cell_type": "code",
      "source": "matrix.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "530cfc4ab817e7f198e0d87342e236b333c0dbf2",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "90ed6ecd407af950d5fb4c449e17abb4ab46d2c8",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "Validation strategy is 34 month for the test set, 33 month for the validation set and 13-33 months for the train."
    },
    {
      "metadata": {
        "_uuid": "a1da8971ae5f0f30c6247241e8b664486589233d",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "X_train = matrix[matrix.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = matrix[matrix.date_block_num < 33]['item_cnt_month']\nX_valid = matrix[matrix.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = matrix[matrix.date_block_num == 33]['item_cnt_month']\nX_test = matrix[matrix.date_block_num == 34].drop(['item_cnt_month'], axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "69c483089922fe0fd96658175bbbe42314c3ed6e",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "del matrix\ngc.collect();",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "90b15c17387eefb29badf9731b60a9140977464e",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "ts = time.time()\n\nmodel = XGBRegressor(seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\ntime.time() - ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "432a1d971e247fb2a1d3d9ca205edb220eec7a4a",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f264d8e4c102589889fd5535083c25d8efa685f9",
        "scrolled": true,
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "plot_features(model, (12,12))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "469fa5d7740c87a7b301d211d05527d1bf1f188b",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "test  = pd.read_csv('../input/test.csv')\nsubmission = pd.DataFrame({\n    'ID': test['ID'], \n    'item_cnt_month': Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "5d9988f8da8876f74092fbf827ceb6c61dd09d5e"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}