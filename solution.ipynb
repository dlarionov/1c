{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f03379ee467570732ebb2b3d20062fea0584d57d"
   },
   "source": [
    "# Part 1, Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import gc\n",
    "import sys\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "items = pd.read_csv('items.csv')\n",
    "shops = pd.read_csv('shops.csv')\n",
    "cats = pd.read_csv('item_categories.csv')\n",
    "train = pd.read_csv('sales_train.csv.gz', compression='gzip')\n",
    "test  = pd.read_csv('test.csv.gz', compression='gzip').set_index('ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ed7a190645750a818e29a6291ba2553a91764c7c"
   },
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "425d8f2dc08378977b393bf80c5fdcf0fba2c992"
   },
   "source": [
    "There are items with strange prices and sales. After detailed exploration I decided to remove items with price > 100000 and sales > 1001 (1000 is ok)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5a864412fafc3129a3e9bd5bb1f18a7cf0c62935"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.xlim(-100, 3000)\n",
    "sns.boxplot(x=train.item_cnt_day)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.xlim(train.item_price.min(), train.item_price.max()*1.1)\n",
    "sns.boxplot(x=train.item_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7e621535d112603c60aeb2c2f83dbbf96d36b732"
   },
   "outputs": [],
   "source": [
    "train = train[train.item_price<100000]\n",
    "train = train[train.item_cnt_day<1001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d2f99368478e3063b1c379537944e954d7186928"
   },
   "source": [
    "There is one item with the price below zero. Fill it with median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0fc6b90b22fe232f4240ac8f965cc52b3db5526a"
   },
   "outputs": [],
   "source": [
    "median = train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)].item_price.median()\n",
    "train.loc[train.item_price<0, 'item_price'] = median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7da194c285d696b5c6978148bf0143b9b2a7b0c5"
   },
   "source": [
    "Several shops are duplicates of each other (according to its name). Fix train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "00fe91e9c482ea413abd774ff903fe3d152785dd"
   },
   "outputs": [],
   "source": [
    "# Якутск Орджоникидзе, 56\n",
    "train.loc[train.shop_id == 0, 'shop_id'] = 57\n",
    "test.loc[test.shop_id == 0, 'shop_id'] = 57\n",
    "# Якутск ТЦ \"Центральный\"\n",
    "train.loc[train.shop_id == 1, 'shop_id'] = 58\n",
    "test.loc[test.shop_id == 1, 'shop_id'] = 58\n",
    "# Жуковский ул. Чкалова 39м²\n",
    "train.loc[train.shop_id == 10, 'shop_id'] = 11\n",
    "test.loc[test.shop_id == 10, 'shop_id'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a30f0521464e1fa20444e66d24bbdcb76b93f6de"
   },
   "source": [
    "## Shops/Cats/Items preprocessing\n",
    "Observations:\n",
    "* Each shop_name starts with the city name.\n",
    "* Each category contains type and subtype in its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "12fae4c8d0c8f3e817307d1e0ffc6831e9a8d696"
   },
   "outputs": [],
   "source": [
    "shops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\n",
    "shops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\n",
    "shops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\n",
    "shops['city_code'] = LabelEncoder().fit_transform(shops['city'])\n",
    "shops = shops[['shop_id','city_code']]\n",
    "\n",
    "cats['split'] = cats['item_category_name'].str.split('-')\n",
    "cats['type'] = cats['split'].map(lambda x: x[0].strip())\n",
    "cats['type_code'] = LabelEncoder().fit_transform(cats['type'])\n",
    "# if subtype is nan then type\n",
    "cats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n",
    "cats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\n",
    "cats = cats[['item_category_id','type_code', 'subtype_code']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e0da873166e895c2c981a9f38f2647cd3327784"
   },
   "source": [
    "TF-IDF features from item_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7ec93dbe2c4126e6e8f129d4cd30cebb28ead3bf"
   },
   "outputs": [],
   "source": [
    "tfidf = pd.DataFrame(TfidfVectorizer(max_features=12).fit_transform(items['item_name']).toarray(), dtype='float16')\n",
    "tfidf.columns = ['tfidf_'+str(id+1) for id in range(12)]\n",
    "tfidf.index.names = ['item_id']\n",
    "tfidf.reset_index(inplace=True)\n",
    "\n",
    "items.drop(['item_name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "62c5f83fa222595da99294f465ab28e80ce415e9"
   },
   "source": [
    "## Monthly sales\n",
    "Test set is a product of some shops and items within 34 month. There are 5100 items * 42 shops = 214200 pairs. 363 items are new compared to train. Hence, for the most of the items in the test set target value should be zero. \n",
    "In the other hand train set contains only pairs which were sold or returned in the past. Tha main idea is to calculate monthly sales and extend it with zero sales for each unique pair within the month. This way train data will be similar to test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb69350aef2c28cdb619e2532de1e24ab3c43899"
   },
   "source": [
    "len(list(set(test.item_id) - set(test.item_id).intersection(set(train.item_id)))), len(list(set(test.item_id))), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7626c7455ea71b65894c6c866519df15080fa2ac"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "matrix = []\n",
    "cols = ['date_block_num','shop_id','item_id']\n",
    "for i in range(34):\n",
    "    sales = train[train.date_block_num==i]\n",
    "    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n",
    "    \n",
    "matrix = pd.DataFrame(np.vstack(matrix), columns=cols)\n",
    "matrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\n",
    "matrix['shop_id'] = matrix['shop_id'].astype(np.int8)\n",
    "matrix['item_id'] = matrix['item_id'].astype(np.int16)\n",
    "matrix.sort_values(cols,inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "867e91a7570dd78b4834f4f1a166e58f80b63f93"
   },
   "source": [
    "Aggregate train set by shop/item pairs to calculate targets sum and count.Clip target value as (0,20)\n",
    "\n",
    "<b>NB</b> I use floats instead of ints for item_cnt_month and orders to avoid downcasting it after concatination with the test set later. If it would be int16, after concatination with NaN values it becomes int64, but foat16 becomes float16 even with NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['revenue'] = train['item_price'] *  train['item_cnt_day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7dd27181918fc7df89676e24d72130d183929d2d"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "train_agg = train.groupby(['date_block_num','shop_id','item_id']).agg({\n",
    "    'item_cnt_day': ['sum', 'count'],\n",
    "    'revenue': ['sum']\n",
    "})\n",
    "train_agg.columns = ['item_cnt_month', 'orders', 'revenue']\n",
    "train_agg.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, train_agg, on=cols, how='left')\n",
    "matrix['item_cnt_month'] = (matrix['item_cnt_month']\n",
    "                                .fillna(0)\n",
    "                                .clip(0,20) # NB clip target here\n",
    "                                .astype(np.float16))\n",
    "matrix['orders'] = matrix['orders'].fillna(0).astype(np.float16)\n",
    "matrix['revenue'] = matrix['revenue'].fillna(0).astype(np.float32)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['date_block_num'] = 34\n",
    "test['date_block_num'] = test['date_block_num'].astype(np.int8)\n",
    "test['shop_id'] = test['shop_id'].astype(np.int8)\n",
    "test['item_id'] = test['item_id'].astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "matrix = pd.concat([matrix, test], ignore_index=True, keys=cols) # there is no sort=False in python 3.5, columns are mixed\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shops/Items/Cats features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "matrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\n",
    "matrix = pd.merge(matrix, items, on=['item_id'], how='left')\n",
    "matrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\n",
    "matrix['city_code'] = matrix['city_code'].astype(np.int8)\n",
    "matrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\n",
    "matrix['type_code'] = matrix['type_code'].astype(np.int8)\n",
    "matrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\n",
    "matrix.fillna(0, inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traget lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_feature(df, lags, col):\n",
    "    tmp = df[['date_block_num','shop_id','item_id',col]]\n",
    "    for i in lags:\n",
    "        shifted = tmp.copy()\n",
    "        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n",
    "        shifted['date_block_num'] += i\n",
    "        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "matrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')\n",
    "matrix = lag_feature(matrix, [1,2,3], 'orders')\n",
    "matrix = lag_feature(matrix, [1,2,3], 'revenue')\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c67bf4dbcef884ffe9d19c65d37bc4de1f287ef6"
   },
   "source": [
    "## Mean encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num']).agg({\n",
    "    'item_cnt_month': ['mean'],\n",
    "    'orders': ['mean'],\n",
    "    'revenue': ['mean'],\n",
    "})\n",
    "group.columns = [ 'date_avg_item_cnt', 'date_avg_orders', 'date_avg_revenue' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\n",
    "matrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\n",
    "matrix['date_avg_orders'] = matrix['date_avg_orders'].astype(np.float16)\n",
    "matrix['date_avg_revenue'] = matrix['date_avg_revenue'].astype(np.float32)\n",
    "\n",
    "matrix = lag_feature(matrix, [1], 'date_avg_item_cnt')\n",
    "matrix = lag_feature(matrix, [1], 'date_avg_orders')\n",
    "matrix = lag_feature(matrix, [1], 'date_avg_revenue')\n",
    "\n",
    "matrix.drop(['date_avg_item_cnt','date_avg_orders','date_avg_revenue'], axis=1, inplace=True)\n",
    "\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fc9166c4e678ebb99d03566f1751b7d4b5c690d2"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'item_id']).agg({\n",
    "    'item_cnt_month': ['mean' ],\n",
    "    'orders': ['sum']\n",
    "})\n",
    "group.columns = [ 'date_item_avg_item_cnt', 'date_item_sum_orders' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\n",
    "matrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\n",
    "matrix['date_item_sum_orders'] = matrix['date_item_sum_orders'].astype(np.float16)\n",
    "\n",
    "matrix = lag_feature(matrix, [1,2,3,6,12], 'date_item_avg_item_cnt')\n",
    "matrix = lag_feature(matrix, [1,2,3,6,12], 'date_item_sum_orders')\n",
    "\n",
    "matrix.drop(['date_item_avg_item_cnt','date_item_sum_orders'], axis=1, inplace=True)\n",
    "\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'shop_id']).agg({\n",
    "    'item_cnt_month': ['mean' ],\n",
    "    'orders': ['sum']\n",
    "})\n",
    "group.columns = [ 'date_shop_avg_item_cnt', 'date_shop_sum_orders' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\n",
    "matrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\n",
    "matrix['date_shop_sum_orders'] = matrix['date_shop_sum_orders'].astype(np.float16)\n",
    "\n",
    "matrix = lag_feature(matrix, [1,2,3,6,12], 'date_shop_avg_item_cnt')\n",
    "matrix = lag_feature(matrix, [1,2,3,6,12], 'date_shop_sum_orders')\n",
    "\n",
    "matrix.drop(['date_shop_avg_item_cnt','date_shop_sum_orders'], axis=1, inplace=True)\n",
    "\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'item_category_id']).agg({\n",
    "    'item_cnt_month': ['mean'],\n",
    "    'orders': ['sum']\n",
    "})\n",
    "group.columns = [ 'date_cat_avg_item_cnt', 'date_cat_sum_orders' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\n",
    "matrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\n",
    "matrix['date_cat_sum_orders'] = matrix['date_cat_sum_orders'].astype(np.float16)\n",
    "\n",
    "matrix = lag_feature(matrix, [1,2,3], 'date_cat_avg_item_cnt')\n",
    "matrix = lag_feature(matrix, [1,2,3], 'date_cat_sum_orders')\n",
    "\n",
    "matrix.drop(['date_cat_avg_item_cnt','date_cat_sum_orders'], axis=1, inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "group = train.groupby(['item_id']).agg({'item_price': ['mean']})\n",
    "group.columns = ['item_avg_item_price']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['item_id'], how='left')\n",
    "matrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "group = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\n",
    "group.columns = ['date_item_avg_item_price']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\n",
    "matrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n",
    "\n",
    "lags = [1,2,3,4,5,6]\n",
    "matrix = lag_feature(matrix, [1,2,3,4,5,6], 'date_item_avg_item_price')\n",
    "\n",
    "for i in lags:\n",
    "    matrix['delta_price_lag_'+str(i)] = \\\n",
    "        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) / matrix['item_avg_item_price']\n",
    "\n",
    "def select_trend(row):\n",
    "    for i in lags:\n",
    "        if row['delta_price_lag_'+str(i)]:\n",
    "            return row['delta_price_lag_'+str(i)]\n",
    "    return 0\n",
    "    \n",
    "matrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\n",
    "\n",
    "# https://stackoverflow.com/questions/31828240/first-non-null-value-per-row-from-a-list-of-pandas-columns/31828559\n",
    "# matrix['price_trend'] = matrix[['delta_price_lag_1','delta_price_lag_2','delta_price_lag_3']].bfill(axis=1).iloc[:, 0]\n",
    "# Invalid dtype for backfill_2d [float16]\n",
    "\n",
    "fetures_to_drop = ['date_item_avg_item_price']\n",
    "for i in lags:\n",
    "    fetures_to_drop += 'date_item_avg_item_price_lag_'+str(i)\n",
    "    fetures_to_drop += 'delta_price_lag_'+str(i)\n",
    "\n",
    "matrix.drop(fetures_to_drop, axis=1, inplace=True)\n",
    "\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "group = train_agg.groupby(['shop_id', 'item_id']).agg({\n",
    "    'item_cnt_month': ['mean'],\n",
    "    'orders': ['mean'],\n",
    "    'revenue': ['mean'],\n",
    "})\n",
    "group.columns = [ 'shop_item_avg_item_cnt', 'shop_item_avg_orders', 'shop_item_avg_revenue' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['shop_id','item_id'], how='left')\n",
    "matrix['shop_item_avg_item_cnt'] = matrix['shop_item_avg_item_cnt'].astype(np.float16)\n",
    "matrix['shop_item_avg_orders'] = matrix['shop_item_avg_orders'].astype(np.float16)\n",
    "matrix['shop_item_avg_revenue'] = matrix['shop_item_avg_revenue'].astype(np.float32)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Month (0 - Jan, 11 - Dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bb521e1f33d4124a3b90b47447bdb29150770b6e"
   },
   "outputs": [],
   "source": [
    "matrix['month'] = matrix['date_block_num'] % 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b4dc4d2ff86483989c4b74fc02a0d01ca68a5c75"
   },
   "source": [
    "Number of days in a month. There are no leap years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e23f0201056b73368e3b70d4c36c6bb9e4a55291"
   },
   "outputs": [],
   "source": [
    "days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\n",
    "matrix['days'] = matrix['month'].map(days).astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c096e86eb0043c0f6eeb899de24e28ca4c4e044"
   },
   "source": [
    "Months since last sale for each shop/item pair and for item only. I use programing approach.\n",
    "\n",
    "Create hash table with key equals to {shop_id,item_id} and value equals date_block_num. Iterate data from the top. Foreach row if {row.shop_id,item_id} is not present in the table, then add to the table and set its value to row.date_blocl_num. if hash table contains key, then calculate the difference beteween cached value and row.date_block_num."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3458a7056c963167760921417d1f863f074f2b39"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "cache = {}\n",
    "matrix['item_shop_last_sale'] = -1\n",
    "matrix['item_shop_last_sale'] = matrix['item_shop_last_sale'].astype(np.int8)\n",
    "for idx, row in matrix.iterrows():    \n",
    "    key = str(row.item_id)+' '+str(row.shop_id)\n",
    "    if key not in cache:\n",
    "        if row.item_cnt_month!=0:\n",
    "            cache[key] = row.date_block_num\n",
    "    else:\n",
    "        last_date_block_num = cache[key]\n",
    "        matrix.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n",
    "        cache[key] = row.date_block_num         \n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "28b29fae3906d870b4dc3064a7f359b6d3abf623"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "cache = {}\n",
    "matrix['item_last_sale'] = -1\n",
    "matrix['item_last_sale'] = matrix['item_last_sale'].astype(np.int8)\n",
    "for idx, row in matrix.iterrows():    \n",
    "    key = row.item_id\n",
    "    if key not in cache:\n",
    "        if row.item_cnt_month!=0:\n",
    "            cache[key] = row.date_block_num\n",
    "    else:\n",
    "        last_date_block_num = cache[key]\n",
    "        if row.date_block_num>last_date_block_num:\n",
    "            matrix.at[idx, 'item_last_sale'] = row.date_block_num - last_date_block_num\n",
    "            cache[key] = row.date_block_num         \n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Months since first sale for each shop/item pair and for item only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "matrix['item_shop_first_sale'] = matrix['date_block_num'] \\\n",
    "    - matrix.groupby(['item_id', 'shop_id'])['date_block_num'].transform('min')\n",
    "matrix['item_first_sale'] = matrix['date_block_num'] \\\n",
    "    - matrix.groupby('item_id')['date_block_num'].transform('min')\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c60aeb05f15e7d9cec69dcc6ccae8fc1f04b5242"
   },
   "source": [
    "TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "39820acdd37dc5bf04e7b65b17a1aea1542799e2"
   },
   "outputs": [],
   "source": [
    "# data = pd.merge(data, tfidf, on='item_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump feature matrix to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix.to_pickle('matrix.pickle')\n",
    "\n",
    "import pickle\n",
    "matrix = pickle.load(open('matrix.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the using 12 as lag value drop first 12 months. Also drop all the columns with this month calculated values (other words which can not be calcucated for the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "data = data[data.date_block_num > 11]\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ts = time.time()\n",
    "def fill_na(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any():\n",
    "            if ('item_cnt' in col):\n",
    "                df[col].fillna(0, inplace=True)\n",
    "            if ('orders' in col):\n",
    "                df[col].fillna(0, inplace=True)\n",
    "            if ('item_price' in col):\n",
    "                df[col].fillna(df[col].median(), inplace=True)\n",
    "    return df\n",
    "\n",
    "data = fill_na(data)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "71c40e5ba3710e43d6d60c3e82aaa326c44d4108"
   },
   "source": [
    "# Part 2, xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "530cfc4ab817e7f198e0d87342e236b333c0dbf2"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "\n",
    "def plot_features(booster, figsize):    \n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    return plot_importance(booster=booster, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90ed6ecd407af950d5fb4c449e17abb4ab46d2c8",
    "collapsed": true
   },
   "source": [
    "Validation strategy is 34 month for the test set, 33 month for the validation set and 13-33 months for the train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a1da8971ae5f0f30c6247241e8b664486589233d"
   },
   "outputs": [],
   "source": [
    "X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\n",
    "Y_train = data[data.date_block_num < 33]['item_cnt_month']\n",
    "X_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\n",
    "Y_valid = data[data.date_block_num == 33]['item_cnt_month']\n",
    "X_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69c483089922fe0fd96658175bbbe42314c3ed6e"
   },
   "outputs": [],
   "source": [
    "del data\n",
    "del cache\n",
    "del group\n",
    "del tfidf\n",
    "del items\n",
    "del shops\n",
    "del train\n",
    "del test\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90b15c17387eefb29badf9731b60a9140977464e"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "\n",
    "model = XGBRegressor(\n",
    "    max_depth=12,\n",
    "    min_child_weight=300, \n",
    "    colsample_bytree=0.8, \n",
    "    colsample_bylevel=0.8, \n",
    "    subsample=0.8, \n",
    "    eta=0.3, \n",
    "    num_round=1000,\n",
    "    seed=42)\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    eval_metric=\"rmse\", \n",
    "    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "    verbose=True, \n",
    "    early_stopping_rounds = 10)\n",
    "\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "432a1d971e247fb2a1d3d9ca205edb220eec7a4a"
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_valid).clip(0, 20)\n",
    "Y_test = model.predict(X_test).clip(0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f264d8e4c102589889fd5535083c25d8efa685f9"
   },
   "outputs": [],
   "source": [
    "plot_features(model, (12,42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "469fa5d7740c87a7b301d211d05527d1bf1f188b"
   },
   "outputs": [],
   "source": [
    "test  = pd.read_csv('../input/test.csv')\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test['ID'], \n",
    "    'item_cnt_month': Y_test\n",
    "})\n",
    "submission.to_csv('xgb_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
